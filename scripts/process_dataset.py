import argparse
import multiprocessing
import os
import sys
import traceback
from pathlib import Path
from typing import Dict, List

import dgl
import joblib as J
import networkx as nx
import torch
from androguard.misc import AnalyzeAPK
from androguard.core.bytecodes.dvm import Instruction

attributes = ['external', 'entrypoint', 'native', 'public', 'static', 'codesize', 'api_package', 'opcodes']
package_directory = os.path.dirname(os.path.abspath(__file__))


def get_api_list() -> Dict[str, int]:
    apis = open(Path(package_directory).parent / "metadata" / "api.list").readlines()
    return {x.strip(): i for i, x in enumerate(apis)}


def get_opcode_mapping() -> Dict[str, int]:
    mapping = {x: i for i, x in enumerate(['nop', 'mov', 'return',
                                           'const', 'monitor', 'check-cast', 'instanceof', 'new',
                                           'fill', 'throw', 'goto/switch', 'cmp', 'if', 'unused',
                                           'arrayop', 'instanceop', 'staticop', 'invoke',
                                           'unaryop', 'binop', 'inline'])}
    mapping['invalid'] = -1
    return mapping


opcode_mapping = get_opcode_mapping()


def get_instruction_type(instr: Instruction) -> str:
    value = instr.get_op_value()
    if 0x00 == value:
        return 'nop'
    elif 0x01 <= value <= 0x0D:
        return 'mov'
    elif 0x0E <= value <= 0x11:
        return 'return'
    elif 0x12 <= value <= 0x1C:
        return 'const'
    elif 0x1D <= value <= 0x1E:
        return 'monitor'
    elif 0x1F == value:
        return 'check-cast'
    elif 0x20 == value:
        return 'instanceof'
    elif 0x22 <= value <= 0x23:
        return 'new'
    elif 0x24 <= value <= 0x26:
        return 'fill'
    elif 0x27 == value:
        return 'throw'
    elif 0x28 <= value <= 0x2C:
        return 'goto/switch'
    elif 0x2D <= value <= 0x31:
        return 'cmp'
    elif 0x32 <= value <= 0x3D:
        return 'if'
    elif (0x3E <= value <= 0x43) or (value == 0x73) or (0x79 <= value <= 0x7A) or (0xE3 <= value <= 0xED):
        return 'unused'
    elif (0x44 <= value <= 0x51) or (value == 0x21):
        return 'arrayop'
    elif (0x52 <= value <= 0x5F) or (0xF2 <= value <= 0xF7):
        return 'instanceop'
    elif 0x60 <= value <= 0x6D:
        return 'staticop'
    elif (0x6E <= value <= 0x72) or (0x74 <= value <= 0x78) or (0xF0 == value) or (0xF8 <= value <= 0xFB):
        return 'invoke'
    elif 0x7B <= value <= 0x8F:
        return 'unaryop'
    elif 0x90 <= value <= 0xE2:
        return 'binop'
    elif 0xEE == value:
        return 'inline'
    else:
        return 'invalid'


def mapping_to_bitstring(mapping: List[int]) -> torch.Tensor:
    size = torch.Size([1, len(opcode_mapping) - 1])
    if len(mapping) > 0:
        indices = torch.LongTensor([[0, x] for x in mapping]).t()
        values = torch.ShortTensor([1] * len(mapping))
        tensor = torch.sparse.ShortTensor(indices, values, size)
    else:
        tensor = torch.sparse.ShortTensor(size)
    return tensor.to_dense()


def process_node_attributes(g: dgl.DGLGraph, api_list: Dict[str, int]) -> dgl.DGLGraph:
    nodes = torch.arange(len(g.nodes()))
    api_packages = g.ndata['api_package']
    dim_0 = nodes[torch.where(api_packages >= 0)].view(-1, 1)
    dim_1 = api_packages[torch.where(api_packages >= 0)].view(-1, 1)
    indices = torch.cat([dim_0, dim_1], dim=1).t()
    values = torch.ones(len(dim_0))
    size = torch.Size([len(g.nodes()), len(api_list)])
    g.ndata['api_package'] = torch.sparse.ShortTensor(indices, values, size).to_dense()
    for attribute in ['external', 'entrypoint', 'native', 'public', 'static', 'codesize']:
        g.ndata[attribute] = g.ndata[attribute].view(-1, 1)
    return g


def process(source_file: Path, dest_dir: Path, api_list: Dict[str, int]):
    try:
        file_name = source_file.stem
        _, _, dx = AnalyzeAPK(source_file)
        cg = dx.get_call_graph()
        mappings = {}
        for node in cg.nodes():
            mapping = {"api_package": -1, "opcodes": set()}
            if node.is_external():
                name = '.'.join(map(str, node.full_name.split(';')[0][1:].split('/')[:-2]))
                index = api_list.get(name, -1)
                mapping["api_package"] = index
            else:
                for instr in node.get_method().get_instructions():
                    instruction_type = get_instruction_type(instr)
                    instruction_id = opcode_mapping[instruction_type]
                    if instruction_id >= 0:
                        mapping["opcodes"].add(instruction_id)
            mapping["opcodes"] = mapping_to_bitstring(mapping["opcodes"])
            mappings[node] = mapping
        nx.set_node_attributes(cg, mappings)
        cg = nx.convert_node_labels_to_integers(cg)
        dg = dgl.from_networkx(cg, node_attrs=attributes)
        dg = process_node_attributes(dg, api_list)
        dest_dir = dest_dir / f'{file_name}'
        dgl.data.utils.save_graphs(str(dest_dir), [dg])
        print(f"Processed {source_file}")
    except:
        print(f"Error while processing {source_file}")
        traceback.print_exception(*sys.exc_info())
        return


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Preprocess APK Dataset into Graphs')
    parser.add_argument('--source-dir', help='The directory containing apks', required=True)
    parser.add_argument('--dest-dir', help='The directory to store processed graphs', required=True)
    parser.add_argument('--override', help='Override existing processed files', action='store_true')
    parser.add_argument('--dry', help='Run without actual processing', action='store_true')
    parser.add_argument('--n-jobs', default=multiprocessing.cpu_count(),
                        help='Number of jobs to be used for processing')
    args = parser.parse_args()
    source_dir = Path(args.source_dir)
    if not source_dir.exists():
        raise FileNotFoundError(f'{source_dir} not found')
    dest_dir = Path(args.dest_dir)
    if not dest_dir.exists():
        raise FileNotFoundError(f'{dest_dir} not found')
    n_jobs = args.n_jobs
    if n_jobs < 2:
        print(f"n_jobs={n_jobs} is too less. Switching to number of CPUs in this machine instead")
        n_jobs = multiprocessing.cpu_count()
    api_list = get_api_list()
    files = [x for x in source_dir.iterdir() if x.is_file()]
    source_files = set([x.stem for x in files])
    dest_files = set([x.name for x in dest_dir.iterdir() if x.is_file()])
    unprocessed = [source_dir / f'{x}.apk' for x in source_files - dest_files]
    print(f"Only {len(unprocessed)} out of {len(source_files)} remain to be processed")
    if args.override:
        print(f"--override specified. Ignoring {len(source_files) - len(unprocessed)} processed files")
        unprocessed = [source_dir / f'{x}.apk' for x in source_files]
    print(f"Starting dataset processing with {n_jobs} Jobs")
    if not args.dry:
        J.Parallel(n_jobs=n_jobs)(J.delayed(process)(x, dest_dir, api_list) for x in unprocessed)
    print("DONE")
