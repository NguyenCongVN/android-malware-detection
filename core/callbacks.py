from pytorch_lightning.callbacks import Callback
import torch.nn
from pytorch_memlab import MemReporter
import gc

class MemoryMonitor(Callback):
    def __init__(self, model: torch.nn.Module, frequency: int = 32):
        self.reporter = MemReporter(model)
        self.frequency = frequency

    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx):
        if batch_idx % self.frequency == 0:
            gc.collect()
            self.reporter.report()
            # print("Max memory allocated :", torch.cuda.memory_summary(pl_module.device))

    def setup(self, trainer, pl_module, stage: str):
        print("Setting up")
        self.reporter.report()

