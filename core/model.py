from typing import List, Tuple

import dgl
import pytorch_lightning as pl
import torch
from dgl.nn import Sequential
from dgl.nn.pytorch import SAGEConv
from torch import nn
import torch.nn.functional as F


class MalwareDetector(pl.LightningModule):
    def __init__(self,
                 input_dimension,
                 convolution_dimensions: List[int],
                 readout_weights: str = 'all'):
        super().__init__()
        if readout_weights != 'internal' and readout_weights != 'external' and readout_weights != 'all':
            raise ValueError(f"aggregator_type must be one of 'internal', 'external or 'all'. Got '{readout_weights}'")
        self.save_hyperparameters()
        self.convolution_layers = []
        for dimension in convolution_dimensions:
            self.convolution_layers.append(
                SAGEConv(input_dimension, dimension, aggregator_type='mean', activation=F.relu)
            )
            input_dimension = dimension
        self.convolution_layers = Sequential(*self.convolution_layers)
        self.last_dimension = input_dimension
        self.classify = nn.Linear(self.last_dimension, 1)
        self.loss_func = nn.BCEWithLogitsLoss()

    def forward(self, g: dgl.DGLGraph) -> torch.Tensor:
        if len(g.ndata.keys()) > 0:
            h = torch.cat([g.ndata[x] for x in g.ndata.keys()], dim=1).float()
        else:
            h = (g.in_degrees() + g.out_degrees()).view(-1, 1).float()
        h = self.convolution_layers.forward(g, h)
        g.ndata['h'] = h if len(self.convolution_layers) > 0 else h[0]
        args = ['weight']
        if self.hparams.readout_weights == 'internal':
            g.ndata['weight'] = (1 - g.ndata['external'].float())
        elif self.hparams.readout_weights == 'external':
            g.ndata['weight'] = (g.ndata['external'].float())
        else:  # All
            args = []
        if len(args) > 0 :
            g.ndata['weight'] = g.ndata['weight'].view(-1, 1).expand(-1, self.last_dimension)
        # Calculate graph representation by averaging all the node representations.
        hg = dgl.mean_nodes(g, 'h', *args)
        return self.classify(hg).squeeze()

    def training_step(self, batch: Tuple[dgl.DGLGraph, torch.Tensor], batch_idx: int) -> torch.Tensor:
        bg, label = batch
        prediction = self.forward(bg)
        loss = self.loss_func(prediction, label)
        self.log('train_loss', loss, on_step=False, on_epoch=True, enable_graph=True)
        return loss

    def validation_step(self, batch: Tuple[dgl.DGLGraph, torch.Tensor], batch_idx: int):
        bg, label = batch
        prediction = self.forward(bg)
        loss = self.loss_func(prediction, label)
        self.log('val_loss', loss, on_step=False, on_epoch=True, enable_graph=True)

    def test_step(self, batch: Tuple[dgl.DGLGraph, torch.Tensor], batch_idx: int):
        bg, label = batch
        prediction = self.forward(bg)
        loss = self.loss_func(prediction, label)
        self.log('test_loss', loss, on_step=False, on_epoch=True, enable_graph=True)

    def configure_optimizers(self) -> torch.optim.Adam:
        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)
        return optimizer
